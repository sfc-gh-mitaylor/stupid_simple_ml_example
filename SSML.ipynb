{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "dtklfzcqjntasuspsf34",
   "authorId": "210084618017",
   "authorName": "ADMIN",
   "authorEmail": "michael.taylor@snowflake.com",
   "sessionId": "423a2088-7479-4933-b9fa-535adee499e6",
   "lastEditTime": 1756762964409
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2d9c92-3b3c-4554-ab2e-17b4162e30ae",
   "metadata": {
    "name": "SETUP",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "3775908f-ca36-4846-8f38-5adca39217f2",
   "metadata": {
    "language": "python",
    "name": "general_imports"
   },
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "f1137f0b-ef9e-4c73-807d-32fbe106b9d0",
   "metadata": {
    "language": "python",
    "name": "misc_imports"
   },
   "outputs": [],
   "source": "from snowflake.ml.modeling.ensemble import RandomForestClassifier\nfrom snowflake.ml.modeling import metrics as snowml_metrics\nfrom snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\nfrom snowflake.ml.registry import registry\nfrom snowflake.ml._internal.utils import identifier\n\n# ML packages  \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import f1_score\nfrom xgboost import XGBClassifier",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c695373e-ac74-4b62-a1f1-08206cbd5c81",
   "metadata": {
    "language": "python",
    "name": "iris_data"
   },
   "source": "import pandas as pd\nfrom sklearn import datasets\n\niris = datasets.load_iris()\ndf_iris = pd.DataFrame(iris.data, columns=['SEP_LEN','SEP_WIDTH','PET_LEN','PET_WIDTH'])\ndf_iris['IRIS_TYPE'] = iris.target\ndf_iris = df_iris[df_iris['IRIS_TYPE'] != 2]\ndf_iris",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aad5e26b-00d2-42fc-8fc5-1226645d81b1",
   "metadata": {
    "language": "python",
    "name": "create_ts"
   },
   "outputs": [],
   "source": "from datetime import datetime, timedelta\n# Function to add an artificial timestamp\ndef add_timestamp(df, start_date):\n    df_copy = df.copy()\n    num_rows = len(df_copy)\n    timestamps = [start_date + timedelta(minutes=i) for i in range(num_rows)]\n    df_copy['DUMMY_TS'] = timestamps\n    return df_copy\n\n# Create three different snapshots with different timestamps\ndf1 = add_timestamp(df_iris, datetime(2023, 1, 1, 10, 0, 0))\ndf2 = add_timestamp(df_iris, datetime(2023, 1, 1, 11, 0, 0))\ndf3 = add_timestamp(df_iris, datetime(2023, 1, 1, 12, 0, 0))\n\n# Concatenate the DataFrames\ndf_evolving = pd.concat([df1, df2, df3], ignore_index=True)\n\n# Remove the DUMMY_TS column before upload to avoid type issues\ndf_upload = df_evolving.drop('DUMMY_TS', axis=1)\n\n# Create table without timestamp column first\ntable_name = \"IRIS_EVOLVING_DATA\"\nsession.sql(f\"\"\"\n    CREATE OR REPLACE TABLE {table_name} (\n        IRIS_ID int, \n        SEP_LEN FLOAT,\n        SEP_WIDTH FLOAT,\n        PET_LEN FLOAT,\n        PET_WIDTH FLOAT,\n        IRIS_TYPE INT\n    )\n\"\"\").collect()\n\ndf_upload['IRIS_ID'] = [i + 1 for i in range(len(df_upload))]\n\n# Upload data without timestamp\nsession.write_pandas(df_upload, table_name, auto_create_table=False, overwrite=True)\n\n# Now add the timestamp column in Snowflake using SQL\nsession.sql(f\"\"\"\n    ALTER TABLE {table_name} \n    ADD COLUMN DUMMY_TS TIMESTAMP_NTZ(9)\n\"\"\").collect()\n\n# Create a new table with the timestamp data using a CTE approach\nsession.sql(f\"\"\"\n    CREATE OR REPLACE TABLE {table_name}_WITH_TS AS\n    SELECT \n        IRIS_ID, SEP_LEN, SEP_WIDTH, PET_LEN, PET_WIDTH, IRIS_TYPE,\n        DATEADD(minute, \n                (ROW_NUMBER() OVER (ORDER BY IRIS_ID) - 1), \n                '2023-01-01 10:00:00'::TIMESTAMP_NTZ) AS DUMMY_TS\n    FROM {table_name}\n\"\"\").collect()\n\n# Drop the old table and rename the new one\nsession.sql(f\"DROP TABLE {table_name}\").collect()\nsession.sql(f\"ALTER TABLE {table_name}_WITH_TS RENAME TO {table_name}\").collect()\n\nprint(\"Data uploaded and timestamp column created successfully!\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "75c9072c-7951-4b06-b283-f57f146f502a",
   "metadata": {
    "language": "python",
    "name": "quick_ts_check"
   },
   "outputs": [],
   "source": "session.table('IRIS_EVOLVING_DATA')",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "520f9f24-f19d-4c4c-a377-9bac9fb941e0",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "ALTER TABLE IRIS_EVOLVING_DATA SET CHANGE_TRACKING = TRUE;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f49f1855-d02f-4633-abaa-222a5647cf50",
   "metadata": {
    "name": "FEATURE_GEN",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "17d5e268-997f-4d14-99ea-e94b09635494",
   "metadata": {
    "language": "python",
    "name": "create_feature_Store"
   },
   "outputs": [],
   "source": "fs = FeatureStore(session=session, \n                  database=\"SSML\", \n                  name=\"PUBLIC\", \n                  default_warehouse=\"MBDA_WH\",\n                  creation_mode=CreationMode.CREATE_IF_NOT_EXIST,)\n\nentity = Entity(name=\"IRIS\", join_keys=[\"IRIS_ID\"])\nfs.register_entity(entity)\nfs.list_entities().show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "948e4640-dca5-45a9-b0c0-5756299a61b9",
   "metadata": {
    "language": "python",
    "name": "pull_into_snowparkdf"
   },
   "outputs": [],
   "source": "sdf = session.table('IRIS_EVOLVING_DATA')\nsdf = sdf.withColumn(\"MY_NEW_FEATURE\", sdf[\"SEP_LEN\"] + sdf[\"SEP_WIDTH\"])\nsdf.limit(3).show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9c8f1862-4b18-471d-b958-13154218980c",
   "metadata": {
    "language": "python",
    "name": "create_reg_features"
   },
   "outputs": [],
   "source": "fv = FeatureView(name=\"IRIS_FEATURES\", \n                 entities=[entity], \n                 feature_df=sdf['IRIS_ID', 'SEP_LEN', 'SEP_WIDTH', 'PET_LEN', 'PET_WIDTH', 'MY_NEW_FEATURE'], \n                 refresh_freq=\"1 minute\", \n                 desc=\"iris features\")\n\nfv.attach_feature_desc(\n    {\n        \"SEP_LEN\": \"The Sepal length\",\n        \"SEP_WIDTH\": \"The sepal width\",\n        \"PET_LEN\": \"The petal length\",\n        \"PET_WIDTH\": \"The petal width\",\n        \"MY_NEW_FEATURE\": \"The sepal length + the sepal width\", \n    }\n)\n\nfv = fs.register_feature_view(feature_view=fv, \n                              version=\"V1\", \n                              block=True)\n\nfs.read_feature_view(fv).limit(3).show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "836b761e-b47f-4847-8742-eeae46b4d018",
   "metadata": {
    "language": "python",
    "name": "and_again"
   },
   "outputs": [],
   "source": "sdf_v2 = sdf.withColumn(\"MY_NEW_FEATURE2\", sdf[\"SEP_LEN\"] * sdf[\"SEP_WIDTH\"])\nfv2 = FeatureView(name=\"IRIS_FEATURES\", \n                  entities=[entity],\n                  feature_df=sdf_v2['IRIS_ID', 'MY_NEW_FEATURE2'], \n                  refresh_freq=\"1 minute\", # can also be a cron schedule - * * * * * America/Los_Angeles\n                  desc=\"iris features\")\n\nfv2 = fs.register_feature_view(feature_view=fv2,               \n                               version=\"V2\", \n                               block=True)\n\nfs.read_feature_view(fv2).limit(3).show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2a5549c8-559f-4036-ad86-012da0598a0c",
   "metadata": {
    "name": "MODEL_GEN",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "d303ae47-ad02-4ba2-a3d5-54a56d7f426d",
   "metadata": {
    "language": "python",
    "name": "rehydrate_spine"
   },
   "outputs": [],
   "source": "spine_df = session.table(\"IRIS_EVOLVING_DATA\")\nspine_df = spine_df.select(\"IRIS_ID\", \"IRIS_TYPE\")\ntraining_data = fs.generate_dataset(name=\"MY_DATASET\",\n                                    spine_df=spine_df,\n                                    features=[fv.slice([\"SEP_LEN\", \n                                                        \"SEP_WIDTH\",\n                                                        \"MY_NEW_FEATURE\"]),\n                                              fv2.slice([\"MY_NEW_FEATURE2\"])],)\n\ntraining_data.read.to_snowpark_dataframe().limit(3).show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0cab97c2-a6e0-4cdf-9b07-1bb65ad169c9",
   "metadata": {
    "language": "python",
    "name": "train_iris_model"
   },
   "outputs": [],
   "source": "# Train initial model\nprint(\"ðŸ”„ Training initial churn prediction model...\")\n\n# Define feature columns\ncategorical_cols = []\nnumerical_cols = ['SEP_LEN', 'SEP_WIDTH', 'MY_NEW_FEATURE', 'MY_NEW_FEATURE2']\nfeature_cols = categorical_cols + numerical_cols\ntarget_col = \"IRIS_TYPE\"\n\ndef train_model(feature_df, pipeline):\n    \"\"\"Train XGBoost model for prediction\"\"\"\n    \n    # Convert to pandas\n    train_df = feature_df.to_pandas()\n    \n    # Split data\n    train_data, test_data = train_test_split(train_df, test_size=0.2, random_state=111)\n    \n    # Create preprocessing pipeline\n    preprocessor = ColumnTransformer(\n        transformers=[\n            (\"ordinal\", OrdinalEncoder(), categorical_cols),\n            (\"scaler\", StandardScaler(), numerical_cols)\n        ]\n    )\n\n    # Train model\n    X_train = train_data[feature_cols]\n    y_train = train_data[target_col]\n\n    if pipeline:\n        print('pipeline used')\n        # Create model pipeline\n        pipeline = Pipeline(\n            steps=[ \n                (\"preprocessor\", preprocessor),\n                (\"model\", XGBClassifier(random_state=42))\n            ]\n        )\n           \n        pipeline.fit(X_train, y_train)\n    else:\n        pipeline = XGBClassifier(random_state=42)\n        pipeline.fit(X_train, y_train)\n    \n    # Evaluate on training set\n    train_predictions = pipeline.predict(X_train)\n    train_f1 = f1_score(y_train, train_predictions, average='weighted')\n    \n    # Evaluate on test set\n    X_test = test_data[feature_cols]\n    y_test = test_data[target_col]\n    \n    test_predictions = pipeline.predict(X_test)\n    test_f1 = f1_score(y_test, test_predictions, average='weighted')\n    \n    return {\n        'model': pipeline,\n        'train_f1_score': train_f1,\n        'test_f1_score': test_f1\n    }\n\n# Train the model\ninput_df = training_data.read.to_snowpark_dataframe()\nmodel_result = train_model(input_df, False)\n\nprint(f\"âœ… Model training completed:\")\nprint(f\"   Training F1 Score: {model_result['train_f1_score']:.4f}\")\nprint(f\"   Test F1 Score: {model_result['test_f1_score']:.4f}\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01eee4a0-b774-4165-9e01-2e2d9f83948f",
   "metadata": {
    "language": "python",
    "name": "create_registry"
   },
   "outputs": [],
   "source": "from snowflake.ml.registry import registry\n\nREGISTRY_DATABASE_NAME = \"SSML\"\nREGISTRY_SCHEMA_NAME = \"PUBLIC\"\nmr = registry.Registry(session=session, database_name=REGISTRY_DATABASE_NAME, schema_name=REGISTRY_SCHEMA_NAME)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a0db5a37-7c39-4d58-a86b-0ece5202222e",
   "metadata": {
    "language": "python",
    "name": "register_model"
   },
   "outputs": [],
   "source": "# Register model in Model Registry\nprint(\"ðŸ”„ Registering model in Model Registry...\")\n\n# Log the trained model\nbaseline_model = mr.log_model(\n    model=model_result['model'],\n    model_name=\"IrisDetector\",\n    version_name=\"explainv8\",\n    conda_dependencies=[\"snowflake-ml-python\", \"xgboost\", \"scikit-learn\"],\n    sample_input_data=input_df[feature_cols].limit(100),\n    options={\"enable_explainability\": True},\n    target_platforms=[\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],                \n    comment=\"Baseline model for customer churn detection\"\n)\n\n# Set metrics for the model\nbaseline_model.set_metric(metric_name=\"train_f1_score\", value=model_result['train_f1_score'])\nbaseline_model.set_metric(metric_name=\"test_f1_score\", value=model_result['test_f1_score'])\n\nprint(\"âœ… Baseline model registered and set as default\")\nprint(f\"   Model name: IrisDetector\")\nprint(f\"   Version: baseline\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5e15a1c8-0deb-488a-b5f0-7028c5008355",
   "metadata": {
    "name": "EXPLAIN",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "1ff69fda-85ec-43fb-8115-27224f604328",
   "metadata": {
    "language": "python",
    "name": "retrieve_deploy_model"
   },
   "outputs": [],
   "source": "# mv is a snowflake.ml.model.ModelVersion object\n\nmodel_name = \"IrisDetector\"\nversion_name = \"explainv8\"\nm = mr.get_model(model_name)\nmv = m.version(version_name)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d18fd4e2-3c5b-45cc-ba30-be3ff150808d",
   "metadata": {
    "language": "python",
    "name": "explanations_raw"
   },
   "outputs": [],
   "source": "explanations = mv.run(input_df[feature_cols], function_name=\"explain\")\nexplanations",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eda6aa27-e871-43af-b05c-3cbce3e414e3",
   "metadata": {
    "name": "MODEL_DEPLOY",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "219e98dc-aa86-4369-b007-4e161b62a949",
   "metadata": {
    "language": "python",
    "name": "run_model_wh"
   },
   "outputs": [],
   "source": "remote_prediction = mv.run(feature_df, function_name=\"predict\")\nremote_prediction.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9efe2fd1-0354-4a1b-ad64-2fd096742d05",
   "metadata": {
    "name": "MODEL_MONITOR",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "e99c975a-cfba-44b5-afec-e0ea699babb4",
   "metadata": {
    "language": "python",
    "name": "get_sample_train_set"
   },
   "outputs": [],
   "source": "spine_df = session.table(\"IRIS_EVOLVING_DATA\")\nspine_df = spine_df.select(\"IRIS_ID\", \"IRIS_TYPE\", \"DUMMY_TS\")\ntraining_data = fs.generate_dataset(name=\"MY_DATASET\",\n                                    spine_df=spine_df,\n                                    features=[fv.slice([\"SEP_LEN\", \n                                                        \"SEP_WIDTH\",\n                                                        \"MY_NEW_FEATURE\"]),\n                                              fv2.slice([\"MY_NEW_FEATURE2\"])],)\ntraining_data.read.to_snowpark_dataframe().limit(100).write.save_as_table(\"IRIS_TRAIN\", mode=\"overwrite\")",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f1776597-b031-4380-aadd-c641604bcd9a",
   "metadata": {
    "language": "sql",
    "name": "create_monitor"
   },
   "outputs": [],
   "source": "CREATE OR REPLACE MODEL MONITOR IRIS_MODEL_MONITOR_SQL\nWITH\n    MODEL=IrisDetector\n    VERSION=baseline\n    FUNCTION=predict\n    SOURCE=IRIS_TRAIN\n    BASELINE=IRIS_TRAIN\n    TIMESTAMP_COLUMN=DUMMY_TS\n    ID_COLUMNS=(IRIS_ID)\n    WAREHOUSE=ALERTS_WH\n    REFRESH_INTERVAL='1 min'\n    AGGREGATION_WINDOW='1 day';",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62c913c3-fcaf-494c-bff7-78b065ea3b42",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "show MODEL MONITORS;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e2e14d9-9839-4731-a1f1-1cc201be7ee3",
   "metadata": {
    "language": "python",
    "name": "cell3"
   },
   "outputs": [],
   "source": "iris = datasets.load_iris()\ndf_iris = pd.DataFrame(iris.data, columns=['SEP_LEN','SEP_WIDTH','PET_LEN','PET_WIDTH'])\ndf_iris['IRIS_TYPE'] = iris.target\ndf_iris = df_iris[df_iris['IRIS_TYPE'] == 2]\ndf_iris",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3781c6d-a1e4-45bb-8725-79b4b442ad83",
   "metadata": {
    "name": "SPCS_DEPLOYMENT",
    "collapsed": false
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "id": "e207cc8b-fa5f-49cd-bca9-bae6cbfcf3d0",
   "metadata": {
    "language": "python",
    "name": "spcs_deployment"
   },
   "outputs": [],
   "source": "# Get signature of the inference function in Python\n# mv is a snowflake.ml.model.ModelVersion object\nmv.show_functions()\n\nmv.create_service(service_name=\"myservice\",\n                  service_compute_pool=\"TUTORIAL_COMPUTE_POOL\",\n                  ingress_enabled=True,\n                  gpu_requests=None)",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d200763-c414-4626-94bc-f37d2a9ae235",
   "metadata": {
    "language": "python",
    "name": "spcs_test"
   },
   "outputs": [],
   "source": "mv.run(\n    feature_df,\n    function_name=\"predict\",\n    service_name=\"my_service\")",
   "execution_count": null
  }
 ]
}